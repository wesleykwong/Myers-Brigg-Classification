{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import mpu.ml\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Input, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensures that tweet and user ids do not appear in scientific notation\n",
    "pd.options.display.float_format = '{:.0f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>confirmed_tweet_ids</th>\n",
       "      <th>user_id</th>\n",
       "      <th>mbti</th>\n",
       "      <th>twitter_text</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14222</th>\n",
       "      <td>559463518056222720</td>\n",
       "      <td>27621196</td>\n",
       "      <td>ISFJ</td>\n",
       "      <td>@alicedeee Ich könnte der stundenlang zuhören!...</td>\n",
       "      <td>German</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14668</th>\n",
       "      <td>473510634081239040</td>\n",
       "      <td>855190290</td>\n",
       "      <td>ENFP</td>\n",
       "      <td>@Moaxi @KatrinaJulie kann ich immer noch sehen...</td>\n",
       "      <td>German</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>377531414130229248</td>\n",
       "      <td>133708993</td>\n",
       "      <td>ENFJ</td>\n",
       "      <td>@GerhardMaier fand ich auch damals. Klappt das...</td>\n",
       "      <td>German</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7705</th>\n",
       "      <td>578258496380149760</td>\n",
       "      <td>397366447</td>\n",
       "      <td>ENFP</td>\n",
       "      <td>38 qm für 700 warm ... ich muss verrückt sein ...</td>\n",
       "      <td>German</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1585</th>\n",
       "      <td>662601975658344448</td>\n",
       "      <td>1305617798</td>\n",
       "      <td>INTP</td>\n",
       "      <td>@dilettiert Willkommen in unserer Welt. Liebe ...</td>\n",
       "      <td>German</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13586</th>\n",
       "      <td>680949466871009280</td>\n",
       "      <td>542772869</td>\n",
       "      <td>INTP</td>\n",
       "      <td>accorsi la deve smettere</td>\n",
       "      <td>Dutch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8513</th>\n",
       "      <td>635612601947394048</td>\n",
       "      <td>169452362</td>\n",
       "      <td>INFP</td>\n",
       "      <td>io vado col finale\\nlo faccio\\nchiudo alle 4 m...</td>\n",
       "      <td>Dutch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1371</th>\n",
       "      <td>466726191257636864</td>\n",
       "      <td>98672812</td>\n",
       "      <td>ENTJ</td>\n",
       "      <td>L'evoluzione dell'Universo in una simulazione ...</td>\n",
       "      <td>Dutch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4695</th>\n",
       "      <td>561655376605872064</td>\n",
       "      <td>548470145</td>\n",
       "      <td>ESFJ</td>\n",
       "      <td>E dopo essermi addormentata fra le tue braccia...</td>\n",
       "      <td>Dutch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1551</th>\n",
       "      <td>585890144764461056</td>\n",
       "      <td>81557253</td>\n",
       "      <td>INFP</td>\n",
       "      <td>Uscire sta diventando imbarazzante: ho chiazze...</td>\n",
       "      <td>Dutch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       confirmed_tweet_ids    user_id  mbti  \\\n",
       "14222   559463518056222720   27621196  ISFJ   \n",
       "14668   473510634081239040  855190290  ENFP   \n",
       "889     377531414130229248  133708993  ENFJ   \n",
       "7705    578258496380149760  397366447  ENFP   \n",
       "1585    662601975658344448 1305617798  INTP   \n",
       "...                    ...        ...   ...   \n",
       "13586   680949466871009280  542772869  INTP   \n",
       "8513    635612601947394048  169452362  INFP   \n",
       "1371    466726191257636864   98672812  ENTJ   \n",
       "4695    561655376605872064  548470145  ESFJ   \n",
       "1551    585890144764461056   81557253  INFP   \n",
       "\n",
       "                                            twitter_text language  \n",
       "14222  @alicedeee Ich könnte der stundenlang zuhören!...   German  \n",
       "14668  @Moaxi @KatrinaJulie kann ich immer noch sehen...   German  \n",
       "889    @GerhardMaier fand ich auch damals. Klappt das...   German  \n",
       "7705   38 qm für 700 warm ... ich muss verrückt sein ...   German  \n",
       "1585   @dilettiert Willkommen in unserer Welt. Liebe ...   German  \n",
       "...                                                  ...      ...  \n",
       "13586                           accorsi la deve smettere    Dutch  \n",
       "8513   io vado col finale\\nlo faccio\\nchiudo alle 4 m...    Dutch  \n",
       "1371   L'evoluzione dell'Universo in una simulazione ...    Dutch  \n",
       "4695   E dopo essermi addormentata fra le tue braccia...    Dutch  \n",
       "1551   Uscire sta diventando imbarazzante: ho chiazze...    Dutch  \n",
       "\n",
       "[80000 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('../NLP_MBTI_Classification/twisty_train.csv', index_col=0)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>confirmed_tweet_ids</th>\n",
       "      <th>user_id</th>\n",
       "      <th>mbti</th>\n",
       "      <th>twitter_text</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8856</th>\n",
       "      <td>158679079082659840</td>\n",
       "      <td>62117549</td>\n",
       "      <td>ENTP</td>\n",
       "      <td>Wisst ihr was das beste an #ibes ist? Dass sie...</td>\n",
       "      <td>German</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15218</th>\n",
       "      <td>597014706436612096</td>\n",
       "      <td>17832573</td>\n",
       "      <td>ENFP</td>\n",
       "      <td>Ich hab Connis 7 Minuten letztes Jahr auch geh...</td>\n",
       "      <td>German</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3635</th>\n",
       "      <td>134641950128214000</td>\n",
       "      <td>81568595</td>\n",
       "      <td>ENFP</td>\n",
       "      <td>@fat_jacK47 ja, aber ich wärs nich :D</td>\n",
       "      <td>German</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>242712774025809920</td>\n",
       "      <td>3253641</td>\n",
       "      <td>ESTJ</td>\n",
       "      <td>@Wally44 danke. Ist runtergeladen :)</td>\n",
       "      <td>German</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588</th>\n",
       "      <td>608464865997549568</td>\n",
       "      <td>3091403585</td>\n",
       "      <td>ISTP</td>\n",
       "      <td>@Patienti_A Schlaf gut</td>\n",
       "      <td>German</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13638</th>\n",
       "      <td>642030068894691328</td>\n",
       "      <td>1327151641</td>\n",
       "      <td>ENTP</td>\n",
       "      <td>quel momento in cui stai pedalando tranquillam...</td>\n",
       "      <td>Dutch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18491</th>\n",
       "      <td>260711964781731840</td>\n",
       "      <td>548470145</td>\n",
       "      <td>ESFJ</td>\n",
       "      <td>uomo perfetto????? FA SCHIFOOOOOOOOOOOOOOOOOOO...</td>\n",
       "      <td>Dutch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6374</th>\n",
       "      <td>692631634366038016</td>\n",
       "      <td>201361241</td>\n",
       "      <td>INTJ</td>\n",
       "      <td>@amerutan non ti allarmare, sto accompagnando ...</td>\n",
       "      <td>Dutch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12934</th>\n",
       "      <td>671954187303919616</td>\n",
       "      <td>164412025</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>- insegnarle l'italiano e avevo tipo otto anni...</td>\n",
       "      <td>Dutch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>294216812768423936</td>\n",
       "      <td>8361022</td>\n",
       "      <td>ENTJ</td>\n",
       "      <td>@bisc_otti anche l'anno scorso non l'ho fatta ...</td>\n",
       "      <td>Dutch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       confirmed_tweet_ids    user_id  mbti  \\\n",
       "8856    158679079082659840   62117549  ENTP   \n",
       "15218   597014706436612096   17832573  ENFP   \n",
       "3635    134641950128214000   81568595  ENFP   \n",
       "1065    242712774025809920    3253641  ESTJ   \n",
       "1588    608464865997549568 3091403585  ISTP   \n",
       "...                    ...        ...   ...   \n",
       "13638   642030068894691328 1327151641  ENTP   \n",
       "18491   260711964781731840  548470145  ESFJ   \n",
       "6374    692631634366038016  201361241  INTJ   \n",
       "12934   671954187303919616  164412025  INFJ   \n",
       "1103    294216812768423936    8361022  ENTJ   \n",
       "\n",
       "                                            twitter_text language  \n",
       "8856   Wisst ihr was das beste an #ibes ist? Dass sie...   German  \n",
       "15218  Ich hab Connis 7 Minuten letztes Jahr auch geh...   German  \n",
       "3635               @fat_jacK47 ja, aber ich wärs nich :D   German  \n",
       "1065                @Wally44 danke. Ist runtergeladen :)   German  \n",
       "1588                              @Patienti_A Schlaf gut   German  \n",
       "...                                                  ...      ...  \n",
       "13638  quel momento in cui stai pedalando tranquillam...    Dutch  \n",
       "18491  uomo perfetto????? FA SCHIFOOOOOOOOOOOOOOOOOOO...    Dutch  \n",
       "6374   @amerutan non ti allarmare, sto accompagnando ...    Dutch  \n",
       "12934  - insegnarle l'italiano e avevo tipo otto anni...    Dutch  \n",
       "1103   @bisc_otti anche l'anno scorso non l'ho fatta ...    Dutch  \n",
       "\n",
       "[20000 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('../NLP_MBTI_Classification/twisty_test.csv', index_col=0)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti_num_encoding = {\n",
    "    'ISTJ':0, 'ISFJ':1, 'INFJ':2, 'INTJ':3,\n",
    "    'ISTP':4, 'ISFP':5, 'INFP':6, 'INTP':7,\n",
    "    'ESTP':8, 'ESFP':9, 'ENFP':10, 'ENTP':11,\n",
    "    'ESTJ':12, 'ESFJ':13, 'ENFJ':14, 'ENTJ':15}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['mbti_encoding'] = train['mbti'].apply(lambda x: mbti_num_encoding[x])\n",
    "test['mbti_encoding'] = test['mbti'].apply(lambda x: mbti_num_encoding[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>twitter_text</th>\n",
       "      <th>mbti_encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14222</th>\n",
       "      <td>@alicedeee Ich könnte der stundenlang zuhören!...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14668</th>\n",
       "      <td>@Moaxi @KatrinaJulie kann ich immer noch sehen...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>@GerhardMaier fand ich auch damals. Klappt das...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7705</th>\n",
       "      <td>38 qm für 700 warm ... ich muss verrückt sein ...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1585</th>\n",
       "      <td>@dilettiert Willkommen in unserer Welt. Liebe ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13586</th>\n",
       "      <td>accorsi la deve smettere</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8513</th>\n",
       "      <td>io vado col finale\\nlo faccio\\nchiudo alle 4 m...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1371</th>\n",
       "      <td>L'evoluzione dell'Universo in una simulazione ...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4695</th>\n",
       "      <td>E dopo essermi addormentata fra le tue braccia...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1551</th>\n",
       "      <td>Uscire sta diventando imbarazzante: ho chiazze...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            twitter_text  mbti_encoding\n",
       "14222  @alicedeee Ich könnte der stundenlang zuhören!...              1\n",
       "14668  @Moaxi @KatrinaJulie kann ich immer noch sehen...             10\n",
       "889    @GerhardMaier fand ich auch damals. Klappt das...             14\n",
       "7705   38 qm für 700 warm ... ich muss verrückt sein ...             10\n",
       "1585   @dilettiert Willkommen in unserer Welt. Liebe ...              7\n",
       "...                                                  ...            ...\n",
       "13586                           accorsi la deve smettere              7\n",
       "8513   io vado col finale\\nlo faccio\\nchiudo alle 4 m...              6\n",
       "1371   L'evoluzione dell'Universo in una simulazione ...             15\n",
       "4695   E dopo essermi addormentata fra le tue braccia...             13\n",
       "1551   Uscire sta diventando imbarazzante: ho chiazze...              6\n",
       "\n",
       "[80000 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train[['twitter_text', 'mbti_encoding']]\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>twitter_text</th>\n",
       "      <th>mbti_encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8856</th>\n",
       "      <td>Wisst ihr was das beste an #ibes ist? Dass sie...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15218</th>\n",
       "      <td>Ich hab Connis 7 Minuten letztes Jahr auch geh...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3635</th>\n",
       "      <td>@fat_jacK47 ja, aber ich wärs nich :D</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>@Wally44 danke. Ist runtergeladen :)</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588</th>\n",
       "      <td>@Patienti_A Schlaf gut</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13638</th>\n",
       "      <td>quel momento in cui stai pedalando tranquillam...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18491</th>\n",
       "      <td>uomo perfetto????? FA SCHIFOOOOOOOOOOOOOOOOOOO...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6374</th>\n",
       "      <td>@amerutan non ti allarmare, sto accompagnando ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12934</th>\n",
       "      <td>- insegnarle l'italiano e avevo tipo otto anni...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>@bisc_otti anche l'anno scorso non l'ho fatta ...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            twitter_text  mbti_encoding\n",
       "8856   Wisst ihr was das beste an #ibes ist? Dass sie...             11\n",
       "15218  Ich hab Connis 7 Minuten letztes Jahr auch geh...             10\n",
       "3635               @fat_jacK47 ja, aber ich wärs nich :D             10\n",
       "1065                @Wally44 danke. Ist runtergeladen :)             12\n",
       "1588                              @Patienti_A Schlaf gut              4\n",
       "...                                                  ...            ...\n",
       "13638  quel momento in cui stai pedalando tranquillam...             11\n",
       "18491  uomo perfetto????? FA SCHIFOOOOOOOOOOOOOOOOOOO...             13\n",
       "6374   @amerutan non ti allarmare, sto accompagnando ...              3\n",
       "12934  - insegnarle l'italiano e avevo tipo otto anni...              2\n",
       "1103   @bisc_otti anche l'anno scorso non l'ho fatta ...             15\n",
       "\n",
       "[20000 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = test[['twitter_text', 'mbti_encoding']]\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Multilingual Word Embedding Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aligned Word Vector (fastText): https://fasttext.cc/docs/en/aligned-vectors.html\n",
    "\n",
    "Gensim API: https://radimrehurek.com/gensim/apiref.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_word_limit = 50000 # Limit due to kernal memory constraint\n",
    "model1 = gensim.models.KeyedVectors.load_word2vec_format('wiki.de.align.vec', limit=max_word_limit)\n",
    "model2 = gensim.models.KeyedVectors.load_word2vec_format('wiki.es.align.vec', limit=max_word_limit)\n",
    "model3 = gensim.models.KeyedVectors.load_word2vec_format('wiki.it.align.vec', limit=max_word_limit)\n",
    "model4 = gensim.models.KeyedVectors.load_word2vec_format('wiki.nl.align.vec', limit=max_word_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordvec_embedding_matrix(model):\n",
    "    # initialize embedding matrix and word-to-id map:\n",
    "    embedding_matrix = np.zeros((max_word_limit + 1, 300))       \n",
    "    vocab_dict = {}\n",
    "\n",
    "    # build the embedding matrix and the word-to-id map:\n",
    "    for i, word in enumerate(model.vocab.keys()):\n",
    "        embedding_vector = model[word]\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            vocab_dict[word] = i\n",
    "    \n",
    "    return (embedding_matrix, vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "em1, vd1 = wordvec_embedding_matrix(model1)\n",
    "em2, vd2 = wordvec_embedding_matrix(model2)\n",
    "em3, vd3 = wordvec_embedding_matrix(model3)\n",
    "em4, vd4 = wordvec_embedding_matrix(model4)\n",
    "\n",
    "embedding_matrix = np.vstack([em1, em2, em3, em4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = {**vd1, **vd2, **vd3, **vd4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into twitter text and mbti number encoding\n",
    "X_train = train['twitter_text']\n",
    "y_train = train['mbti_encoding']\n",
    "\n",
    "X_test = test['twitter_text']\n",
    "y_test = test['mbti_encoding']\n",
    "\n",
    "# Convert number encoding to one hot vector\n",
    "#import mpu.ml\n",
    "y_train = np.array(mpu.ml.indices2one_hot(y_train, nb_classes=16))\n",
    "y_test = np.array(mpu.ml.indices2one_hot(y_test, nb_classes=16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max token length: 52\n"
     ]
    }
   ],
   "source": [
    "# Twitter tokenizer specific for tweets\n",
    "# preserve_case=False converts everything to lowercase\n",
    "# strip_handles remove Twitter username handles from text\n",
    "# reduce_len=True replace repeated character sequences of length 3 or greater with sequences of length 3\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "\n",
    "# This is to identify the longest token sequence after the tokenizer has been applied to it\n",
    "# This number will be used for padding and initial configuration of CNN model below\n",
    "#MAX_SEQUENCE_LENGTH = 0\n",
    "#for sentence in train['twitter_text']:\n",
    "#    token_length = len(tokenizer.tokenize(sentence))\n",
    "#    MAX_SEQUENCE_LENGTH = max(MAX_SEQUENCE_LENGTH, token_length)\n",
    "    \n",
    "#for sentence in test['twitter_text']:\n",
    "#    token_length = len(tokenizer.tokenize(sentence))\n",
    "#    MAX_SEQUENCE_LENGTH = max(MAX_SEQUENCE_LENGTH, token_length)\n",
    "\n",
    "# Due to computational limitations, MAX_SEQUENCE_LENGTH has already been precalculated\n",
    "MAX_SEQUENCE_LENGTH = 52\n",
    "print(\"Max token length:\", MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "\n",
    "def sents_to_ids(sentences):\n",
    "    \"\"\"\n",
    "    converting a list of strings to a list of lists of word ids\n",
    "    \"\"\"\n",
    "    max_length = MAX_SEQUENCE_LENGTH\n",
    "    text_ids = np.zeros((1, max_length), dtype=int)\n",
    "    for sentence in sentences:\n",
    "        example = []\n",
    "        for word in tokenizer.tokenize(sentence):\n",
    "            if word in vocab_dict.keys():\n",
    "                example.append(vocab_dict[word])\n",
    "            #else:\n",
    "            #    example.append(0)\n",
    "\n",
    "        example = np.pad(example, (0, max_length-len(example)))\n",
    "        text_ids = np.vstack((text_ids, example))\n",
    "    \n",
    "    text_ids = np.delete(text_ids, 0, axis=0)\n",
    "\n",
    "    return text_ids\n",
    "\n",
    "X_train = sents_to_ids(X_train)\n",
    "X_test = sents_to_ids(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN NLP Model and Accuracy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mbti_accuracy(y_true, y_pred):\n",
    "    # Measures accuracy for mbti classification across 5 accuracy metrics:\n",
    "    # one match, two matches, three matches, perfect match, average match\n",
    "    \n",
    "    # Average match is number of letters match / 4\n",
    "    \n",
    "    # Comparing 'ENFJ' as the true class and 'ENFP' as the predicted class,\n",
    "    # this function returns...\n",
    "    # [1, 1, 1, 0, 0.75]\n",
    "     \n",
    "    # Get index from one hot encoding of y_true\n",
    "    # Get index of highest softmax/probability output in y_pred\n",
    "    y_true_index = np.argmax(y_true, axis=1)\n",
    "    y_pred_index = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    # Use the index to identify the corresponding mbti class\n",
    "    mbti_num_encoding_list = list(mbti_num_encoding)\n",
    "    y_true_mbti = [mbti_num_encoding_list[idx] for idx in y_true_index]\n",
    "    y_pred_mbti = [mbti_num_encoding_list[idx] for idx in y_pred_index]\n",
    "    \n",
    "    one_match = []\n",
    "    two_matches = []\n",
    "    three_matches = []\n",
    "    perfect_match = []\n",
    "    \n",
    "    # Perform mbti accuracy measurements\n",
    "    sum_num_matches = 0\n",
    "    for i in np.arange(len(y_true_mbti)):\n",
    "        num_letter_matches = len(set(y_true_mbti[i]) & set(y_pred_mbti[i]))\n",
    "        \n",
    "        # At least 1 letter match\n",
    "        if num_letter_matches == 1:\n",
    "            one_match += [True]\n",
    "            two_matches += [False]\n",
    "            three_matches += [False]\n",
    "            perfect_match += [False]\n",
    "            \n",
    "        # At least 2 letter matches\n",
    "        elif num_letter_matches == 2:\n",
    "            one_match += [True]\n",
    "            two_matches += [True]\n",
    "            three_matches += [False]\n",
    "            perfect_match += [False]\n",
    "            \n",
    "        # At least 3 letter matches\n",
    "        elif num_letter_matches == 3:\n",
    "            one_match += [True]\n",
    "            two_matches += [True]\n",
    "            three_matches += [True]\n",
    "            perfect_match += [False]\n",
    "           \n",
    "        # Perfect match\n",
    "        else:\n",
    "            one_match += [True]\n",
    "            two_matches += [True]\n",
    "            three_matches += [True]\n",
    "            perfect_match += [True]\n",
    "        \n",
    "    # Average/partial matches\n",
    "        sum_num_matches += num_letter_matches\n",
    "    avg_num_matches = sum_num_matches/(len(y_true_mbti)*4)*100\n",
    "    \n",
    "    return np.round([np.mean(one_match)*100, \n",
    "                     np.mean(two_matches)*100, \n",
    "                     np.mean(three_matches)*100, \n",
    "                     np.mean(perfect_match)*100, \n",
    "                     avg_num_matches], \n",
    "                    2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_array = compute_class_weight('balanced', \n",
    "                       classes=np.arange(16), \n",
    "                       y=np.argmax(y_train, axis=1))\n",
    "weights = dict(zip(np.arange(16), weights_array))\n",
    "\n",
    "embedding_layer = Embedding(embedding_matrix.shape[0],\n",
    "                            embedding_matrix.shape[1],\n",
    "                            embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "def cnn_model(X_train, y_train, under_represented_weighting, optimizer='adam', epochs_input=100,  batch_size_input=64):\n",
    "    \n",
    "    # CNN Model Architecture\n",
    "    tf_model = tf.keras.Sequential()\n",
    "    tf_model.add(embedding_layer)\n",
    "    tf_model.add(tf.keras.layers.Conv1D(\n",
    "                filters=10, \n",
    "                kernel_size=3, \n",
    "                strides=1, \n",
    "                padding='same', \n",
    "                activation='relu', \n",
    "                use_bias=True,\n",
    "                kernel_initializer='glorot_uniform', \n",
    "                bias_initializer='zeros')) \n",
    "    tf_model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "    tf_model.add(Dense(100, activation='relu'))\n",
    "    tf_model.add(Dense(16, activation='sigmoid'))\n",
    "    \n",
    "    tf_model.compile(optimizer=optimizer, loss=CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "    \n",
    "    tf_model.fit(X_train, y_train, \n",
    "                           validation_data=(X_test, y_test), \n",
    "                           class_weight=under_represented_weighting,\n",
    "                           epochs=epochs_input, \n",
    "                           batch_size=batch_size_input)\n",
    "            \n",
    "    return tf_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Training Size Effects on Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "63/63 [==============================] - 7s 117ms/step - loss: 2.7871 - accuracy: 0.0324 - val_loss: 2.7733 - val_accuracy: 0.0321\n",
      "Epoch 2/30\n",
      "63/63 [==============================] - 7s 115ms/step - loss: 2.7808 - accuracy: 0.0437 - val_loss: 2.7725 - val_accuracy: 0.0602\n",
      "Epoch 3/30\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 2.7576 - accuracy: 0.0913 - val_loss: 2.7585 - val_accuracy: 0.0806\n",
      "Epoch 4/30\n",
      "63/63 [==============================] - 7s 116ms/step - loss: 2.7204 - accuracy: 0.1147 - val_loss: 2.7207 - val_accuracy: 0.1070\n",
      "Epoch 5/30\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 2.6927 - accuracy: 0.1316 - val_loss: 2.7088 - val_accuracy: 0.1103\n",
      "Epoch 6/30\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 2.6679 - accuracy: 0.1361 - val_loss: 2.6829 - val_accuracy: 0.1217\n",
      "Epoch 7/30\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 2.6398 - accuracy: 0.1358 - val_loss: 2.7498 - val_accuracy: 0.0941\n",
      "Epoch 8/30\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 2.6162 - accuracy: 0.1336 - val_loss: 2.7324 - val_accuracy: 0.0927\n",
      "Epoch 9/30\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 2.5860 - accuracy: 0.1347 - val_loss: 2.7296 - val_accuracy: 0.0949\n",
      "Epoch 10/30\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 2.5598 - accuracy: 0.1359 - val_loss: 2.7556 - val_accuracy: 0.0817\n",
      "Epoch 11/30\n",
      "63/63 [==============================] - 7s 113ms/step - loss: 2.5271 - accuracy: 0.1322 - val_loss: 2.7446 - val_accuracy: 0.0908\n",
      "Epoch 12/30\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 2.5004 - accuracy: 0.1319 - val_loss: 2.7336 - val_accuracy: 0.0765\n",
      "Epoch 13/30\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 2.4741 - accuracy: 0.1324 - val_loss: 2.7694 - val_accuracy: 0.0671\n",
      "Epoch 14/30\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 2.4466 - accuracy: 0.1325 - val_loss: 2.7754 - val_accuracy: 0.0781\n",
      "Epoch 15/30\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 2.4208 - accuracy: 0.1364 - val_loss: 2.7872 - val_accuracy: 0.0636\n",
      "Epoch 16/30\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 2.3965 - accuracy: 0.1334 - val_loss: 2.7535 - val_accuracy: 0.0759\n",
      "Epoch 17/30\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 2.3802 - accuracy: 0.1348 - val_loss: 2.8309 - val_accuracy: 0.0516\n",
      "Epoch 18/30\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 2.3571 - accuracy: 0.1373 - val_loss: 2.7288 - val_accuracy: 0.0744\n",
      "Epoch 19/30\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 2.3397 - accuracy: 0.1384 - val_loss: 2.7842 - val_accuracy: 0.0659\n",
      "Epoch 20/30\n",
      "63/63 [==============================] - 7s 113ms/step - loss: 2.3230 - accuracy: 0.1406 - val_loss: 2.7861 - val_accuracy: 0.0715\n",
      "Epoch 21/30\n",
      "63/63 [==============================] - 7s 113ms/step - loss: 2.3024 - accuracy: 0.1435 - val_loss: 2.8001 - val_accuracy: 0.0617\n",
      "Epoch 22/30\n",
      "63/63 [==============================] - 7s 113ms/step - loss: 2.2858 - accuracy: 0.1418 - val_loss: 2.8746 - val_accuracy: 0.0546\n",
      "Epoch 23/30\n",
      "63/63 [==============================] - 7s 113ms/step - loss: 2.2726 - accuracy: 0.1441 - val_loss: 2.7989 - val_accuracy: 0.0676\n",
      "Epoch 24/30\n",
      "63/63 [==============================] - 7s 113ms/step - loss: 2.2576 - accuracy: 0.1464 - val_loss: 2.8441 - val_accuracy: 0.0561\n",
      "Epoch 25/30\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 2.2445 - accuracy: 0.1476 - val_loss: 2.8051 - val_accuracy: 0.0686\n",
      "Epoch 26/30\n",
      "63/63 [==============================] - 7s 113ms/step - loss: 2.2263 - accuracy: 0.1536 - val_loss: 2.8200 - val_accuracy: 0.0692\n",
      "Epoch 27/30\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 2.2137 - accuracy: 0.1497 - val_loss: 2.8012 - val_accuracy: 0.0628\n",
      "Epoch 28/30\n",
      "63/63 [==============================] - 7s 113ms/step - loss: 2.2020 - accuracy: 0.1514 - val_loss: 2.8142 - val_accuracy: 0.0645\n",
      "Epoch 29/30\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 2.1873 - accuracy: 0.1506 - val_loss: 2.8508 - val_accuracy: 0.0547\n",
      "Epoch 30/30\n",
      "63/63 [==============================] - 7s 113ms/step - loss: 2.1812 - accuracy: 0.1526 - val_loss: 2.8204 - val_accuracy: 0.0688\n",
      "Epoch 1/30\n",
      "125/125 [==============================] - 13s 101ms/step - loss: 2.7781 - accuracy: 0.0406 - val_loss: 2.7532 - val_accuracy: 0.0975\n",
      "Epoch 2/30\n",
      "125/125 [==============================] - 12s 100ms/step - loss: 2.7405 - accuracy: 0.1110 - val_loss: 2.7337 - val_accuracy: 0.1065\n",
      "Epoch 3/30\n",
      "125/125 [==============================] - 12s 100ms/step - loss: 2.7020 - accuracy: 0.1265 - val_loss: 2.7107 - val_accuracy: 0.1319\n",
      "Epoch 4/30\n",
      "125/125 [==============================] - 12s 100ms/step - loss: 2.6813 - accuracy: 0.1344 - val_loss: 2.7298 - val_accuracy: 0.1182\n",
      "Epoch 5/30\n",
      "125/125 [==============================] - 12s 100ms/step - loss: 2.6634 - accuracy: 0.1350 - val_loss: 2.7223 - val_accuracy: 0.1254\n",
      "Epoch 6/30\n",
      "125/125 [==============================] - 12s 100ms/step - loss: 2.6439 - accuracy: 0.1310 - val_loss: 2.6513 - val_accuracy: 0.1437\n",
      "Epoch 7/30\n",
      "125/125 [==============================] - 12s 100ms/step - loss: 2.6253 - accuracy: 0.1281 - val_loss: 2.6962 - val_accuracy: 0.1169\n",
      "Epoch 8/30\n",
      "125/125 [==============================] - 13s 100ms/step - loss: 2.6027 - accuracy: 0.1247 - val_loss: 2.7515 - val_accuracy: 0.0966\n",
      "Epoch 9/30\n",
      "125/125 [==============================] - 12s 100ms/step - loss: 2.5852 - accuracy: 0.1241 - val_loss: 2.6718 - val_accuracy: 0.1079\n",
      "Epoch 10/30\n",
      "125/125 [==============================] - 12s 100ms/step - loss: 2.5672 - accuracy: 0.1192 - val_loss: 2.7463 - val_accuracy: 0.0698\n",
      "Epoch 11/30\n",
      "125/125 [==============================] - 12s 100ms/step - loss: 2.5459 - accuracy: 0.1182 - val_loss: 2.7825 - val_accuracy: 0.0852\n",
      "Epoch 12/30\n",
      "125/125 [==============================] - 12s 99ms/step - loss: 2.5335 - accuracy: 0.1151 - val_loss: 2.6920 - val_accuracy: 0.0940\n",
      "Epoch 13/30\n",
      "125/125 [==============================] - 12s 100ms/step - loss: 2.5114 - accuracy: 0.1199 - val_loss: 2.7643 - val_accuracy: 0.0659\n",
      "Epoch 14/30\n",
      "125/125 [==============================] - 12s 99ms/step - loss: 2.4974 - accuracy: 0.1136 - val_loss: 2.7153 - val_accuracy: 0.0867\n",
      "Epoch 15/30\n",
      "125/125 [==============================] - 13s 100ms/step - loss: 2.4806 - accuracy: 0.1168 - val_loss: 2.7611 - val_accuracy: 0.0725\n",
      "Epoch 16/30\n",
      "125/125 [==============================] - 12s 100ms/step - loss: 2.4691 - accuracy: 0.1156 - val_loss: 2.7488 - val_accuracy: 0.0849\n",
      "Epoch 17/30\n",
      "125/125 [==============================] - 12s 100ms/step - loss: 2.4590 - accuracy: 0.1159 - val_loss: 2.7494 - val_accuracy: 0.0804\n",
      "Epoch 18/30\n",
      "125/125 [==============================] - 12s 99ms/step - loss: 2.4444 - accuracy: 0.1161 - val_loss: 2.7582 - val_accuracy: 0.0729\n",
      "Epoch 19/30\n",
      "125/125 [==============================] - 12s 100ms/step - loss: 2.4286 - accuracy: 0.1153 - val_loss: 2.7544 - val_accuracy: 0.0764\n",
      "Epoch 20/30\n",
      "125/125 [==============================] - 13s 100ms/step - loss: 2.4207 - accuracy: 0.1172 - val_loss: 2.7537 - val_accuracy: 0.0698\n",
      "Epoch 21/30\n",
      "125/125 [==============================] - 13s 100ms/step - loss: 2.4098 - accuracy: 0.1187 - val_loss: 2.7566 - val_accuracy: 0.0776\n",
      "Epoch 22/30\n",
      "125/125 [==============================] - 12s 100ms/step - loss: 2.3997 - accuracy: 0.1200 - val_loss: 2.7905 - val_accuracy: 0.0684\n",
      "Epoch 23/30\n",
      "125/125 [==============================] - 12s 100ms/step - loss: 2.3892 - accuracy: 0.1208 - val_loss: 2.7559 - val_accuracy: 0.0708\n",
      "Epoch 24/30\n",
      "125/125 [==============================] - 12s 99ms/step - loss: 2.3804 - accuracy: 0.1195 - val_loss: 2.7994 - val_accuracy: 0.0667\n",
      "Epoch 25/30\n",
      "125/125 [==============================] - 12s 100ms/step - loss: 2.3718 - accuracy: 0.1228 - val_loss: 2.7763 - val_accuracy: 0.0749\n",
      "Epoch 26/30\n",
      "125/125 [==============================] - 12s 99ms/step - loss: 2.3641 - accuracy: 0.1230 - val_loss: 2.7842 - val_accuracy: 0.0707\n",
      "Epoch 27/30\n",
      "125/125 [==============================] - 12s 100ms/step - loss: 2.3621 - accuracy: 0.1232 - val_loss: 2.7511 - val_accuracy: 0.0715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30\n",
      "125/125 [==============================] - 12s 100ms/step - loss: 2.3502 - accuracy: 0.1224 - val_loss: 2.7717 - val_accuracy: 0.0707\n",
      "Epoch 29/30\n",
      "125/125 [==============================] - 12s 100ms/step - loss: 2.3457 - accuracy: 0.1254 - val_loss: 2.8528 - val_accuracy: 0.0567\n",
      "Epoch 30/30\n",
      "125/125 [==============================] - 12s 99ms/step - loss: 2.3322 - accuracy: 0.1248 - val_loss: 2.8077 - val_accuracy: 0.0654\n",
      "Epoch 1/30\n",
      "188/188 [==============================] - 18s 96ms/step - loss: 2.7593 - accuracy: 0.0917 - val_loss: 2.7199 - val_accuracy: 0.1142\n",
      "Epoch 2/30\n",
      "188/188 [==============================] - 18s 95ms/step - loss: 2.7004 - accuracy: 0.1284 - val_loss: 2.6990 - val_accuracy: 0.1321\n",
      "Epoch 3/30\n",
      "188/188 [==============================] - 18s 95ms/step - loss: 2.6719 - accuracy: 0.1321 - val_loss: 2.7059 - val_accuracy: 0.1242\n",
      "Epoch 4/30\n",
      "188/188 [==============================] - 18s 95ms/step - loss: 2.6501 - accuracy: 0.1326 - val_loss: 2.7147 - val_accuracy: 0.1147\n",
      "Epoch 5/30\n",
      "188/188 [==============================] - 18s 95ms/step - loss: 2.6309 - accuracy: 0.1274 - val_loss: 2.7378 - val_accuracy: 0.1007\n",
      "Epoch 6/30\n",
      "188/188 [==============================] - 18s 96ms/step - loss: 2.6112 - accuracy: 0.1225 - val_loss: 2.7027 - val_accuracy: 0.1031\n",
      "Epoch 7/30\n",
      "188/188 [==============================] - 18s 96ms/step - loss: 2.5898 - accuracy: 0.1190 - val_loss: 2.7005 - val_accuracy: 0.1000\n",
      "Epoch 8/30\n",
      "188/188 [==============================] - 18s 96ms/step - loss: 2.5701 - accuracy: 0.1204 - val_loss: 2.6625 - val_accuracy: 0.1051\n",
      "Epoch 9/30\n",
      "188/188 [==============================] - 18s 96ms/step - loss: 2.5528 - accuracy: 0.1161 - val_loss: 2.7019 - val_accuracy: 0.0921\n",
      "Epoch 10/30\n",
      "188/188 [==============================] - 18s 95ms/step - loss: 2.5356 - accuracy: 0.1172 - val_loss: 2.7767 - val_accuracy: 0.0787\n",
      "Epoch 11/30\n",
      "188/188 [==============================] - 18s 95ms/step - loss: 2.5182 - accuracy: 0.1171 - val_loss: 2.6926 - val_accuracy: 0.0963\n",
      "Epoch 12/30\n",
      "188/188 [==============================] - 18s 96ms/step - loss: 2.5025 - accuracy: 0.1161 - val_loss: 2.7412 - val_accuracy: 0.0798\n",
      "Epoch 13/30\n",
      "188/188 [==============================] - 18s 97ms/step - loss: 2.4902 - accuracy: 0.1172 - val_loss: 2.7197 - val_accuracy: 0.0778\n",
      "Epoch 14/30\n",
      "188/188 [==============================] - 18s 96ms/step - loss: 2.4772 - accuracy: 0.1184 - val_loss: 2.7104 - val_accuracy: 0.0922\n",
      "Epoch 15/30\n",
      "188/188 [==============================] - 18s 97ms/step - loss: 2.4619 - accuracy: 0.1209 - val_loss: 2.7813 - val_accuracy: 0.0733\n",
      "Epoch 16/30\n",
      "188/188 [==============================] - 18s 97ms/step - loss: 2.4530 - accuracy: 0.1200 - val_loss: 2.7471 - val_accuracy: 0.0737\n",
      "Epoch 17/30\n",
      "188/188 [==============================] - 18s 96ms/step - loss: 2.4429 - accuracy: 0.1218 - val_loss: 2.7971 - val_accuracy: 0.0624\n",
      "Epoch 18/30\n",
      "188/188 [==============================] - 18s 96ms/step - loss: 2.4354 - accuracy: 0.1238 - val_loss: 2.7295 - val_accuracy: 0.0772\n",
      "Epoch 19/30\n",
      "188/188 [==============================] - 18s 96ms/step - loss: 2.4247 - accuracy: 0.1201 - val_loss: 2.7807 - val_accuracy: 0.0650\n",
      "Epoch 20/30\n",
      "188/188 [==============================] - 18s 96ms/step - loss: 2.4158 - accuracy: 0.1214 - val_loss: 2.7204 - val_accuracy: 0.0852\n",
      "Epoch 21/30\n",
      "188/188 [==============================] - 18s 96ms/step - loss: 2.4078 - accuracy: 0.1237 - val_loss: 2.7960 - val_accuracy: 0.0689\n",
      "Epoch 22/30\n",
      "188/188 [==============================] - 18s 96ms/step - loss: 2.4003 - accuracy: 0.1233 - val_loss: 2.7296 - val_accuracy: 0.0877\n",
      "Epoch 23/30\n",
      "188/188 [==============================] - 18s 97ms/step - loss: 2.3909 - accuracy: 0.1255 - val_loss: 2.7862 - val_accuracy: 0.0755\n",
      "Epoch 24/30\n",
      "188/188 [==============================] - 18s 97ms/step - loss: 2.3845 - accuracy: 0.1244 - val_loss: 2.8025 - val_accuracy: 0.0732\n",
      "Epoch 25/30\n",
      "188/188 [==============================] - 18s 96ms/step - loss: 2.3784 - accuracy: 0.1256 - val_loss: 2.7942 - val_accuracy: 0.0736\n",
      "Epoch 26/30\n",
      "188/188 [==============================] - 18s 96ms/step - loss: 2.3719 - accuracy: 0.1254 - val_loss: 2.7947 - val_accuracy: 0.0786\n",
      "Epoch 27/30\n",
      "188/188 [==============================] - 18s 96ms/step - loss: 2.3685 - accuracy: 0.1255 - val_loss: 2.7903 - val_accuracy: 0.0735\n",
      "Epoch 28/30\n",
      "188/188 [==============================] - 18s 96ms/step - loss: 2.3622 - accuracy: 0.1272 - val_loss: 2.7792 - val_accuracy: 0.0748\n",
      "Epoch 29/30\n",
      "188/188 [==============================] - 18s 97ms/step - loss: 2.3537 - accuracy: 0.1277 - val_loss: 2.8398 - val_accuracy: 0.0620\n",
      "Epoch 30/30\n",
      "188/188 [==============================] - 18s 96ms/step - loss: 2.3485 - accuracy: 0.1285 - val_loss: 2.8300 - val_accuracy: 0.0652\n",
      "Epoch 1/30\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 2.7497 - accuracy: 0.0929 - val_loss: 2.7170 - val_accuracy: 0.1173\n",
      "Epoch 2/30\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 2.6943 - accuracy: 0.1212 - val_loss: 2.6939 - val_accuracy: 0.1429\n",
      "Epoch 3/30\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 2.6662 - accuracy: 0.1311 - val_loss: 2.6807 - val_accuracy: 0.1207\n",
      "Epoch 4/30\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 2.6431 - accuracy: 0.1281 - val_loss: 2.6763 - val_accuracy: 0.1251\n",
      "Epoch 5/30\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 2.6214 - accuracy: 0.1283 - val_loss: 2.6693 - val_accuracy: 0.1269\n",
      "Epoch 6/30\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 2.6041 - accuracy: 0.1264 - val_loss: 2.6760 - val_accuracy: 0.1180\n",
      "Epoch 7/30\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 2.5872 - accuracy: 0.1246 - val_loss: 2.7203 - val_accuracy: 0.0870\n",
      "Epoch 8/30\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 2.5690 - accuracy: 0.1201 - val_loss: 2.7786 - val_accuracy: 0.0821\n",
      "Epoch 9/30\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 2.5544 - accuracy: 0.1198 - val_loss: 2.6914 - val_accuracy: 0.1059\n",
      "Epoch 10/30\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 2.5423 - accuracy: 0.1200 - val_loss: 2.7220 - val_accuracy: 0.0898\n",
      "Epoch 11/30\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 2.5294 - accuracy: 0.1177 - val_loss: 2.7470 - val_accuracy: 0.0979\n",
      "Epoch 12/30\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 2.5180 - accuracy: 0.1191 - val_loss: 2.7587 - val_accuracy: 0.0887\n",
      "Epoch 13/30\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 2.5067 - accuracy: 0.1187 - val_loss: 2.7067 - val_accuracy: 0.1021\n",
      "Epoch 14/30\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 2.4973 - accuracy: 0.1188 - val_loss: 2.7322 - val_accuracy: 0.0820\n",
      "Epoch 15/30\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 2.4885 - accuracy: 0.1176 - val_loss: 2.7480 - val_accuracy: 0.0879\n",
      "Epoch 16/30\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 2.4789 - accuracy: 0.1200 - val_loss: 2.7465 - val_accuracy: 0.0846\n",
      "Epoch 17/30\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 2.4722 - accuracy: 0.1174 - val_loss: 2.7602 - val_accuracy: 0.0717\n",
      "Epoch 18/30\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 2.4640 - accuracy: 0.1169 - val_loss: 2.6926 - val_accuracy: 0.1055\n",
      "Epoch 19/30\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 2.4582 - accuracy: 0.1174 - val_loss: 2.7266 - val_accuracy: 0.0800\n",
      "Epoch 20/30\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 2.4518 - accuracy: 0.1201 - val_loss: 2.7598 - val_accuracy: 0.0809\n",
      "Epoch 21/30\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 2.4436 - accuracy: 0.1170 - val_loss: 2.7355 - val_accuracy: 0.0936\n",
      "Epoch 22/30\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 2.4391 - accuracy: 0.1203 - val_loss: 2.7511 - val_accuracy: 0.0795\n",
      "Epoch 23/30\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 2.4334 - accuracy: 0.1184 - val_loss: 2.7618 - val_accuracy: 0.0763\n",
      "Epoch 24/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 23s 93ms/step - loss: 2.4279 - accuracy: 0.1192 - val_loss: 2.7629 - val_accuracy: 0.0808\n",
      "Epoch 25/30\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 2.4225 - accuracy: 0.1173 - val_loss: 2.7334 - val_accuracy: 0.0914\n",
      "Epoch 26/30\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 2.4200 - accuracy: 0.1176 - val_loss: 2.7559 - val_accuracy: 0.0718\n",
      "Epoch 27/30\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 2.4157 - accuracy: 0.1179 - val_loss: 2.7749 - val_accuracy: 0.0796\n",
      "Epoch 28/30\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 2.4105 - accuracy: 0.1184 - val_loss: 2.7939 - val_accuracy: 0.0873\n",
      "Epoch 29/30\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 2.4089 - accuracy: 0.1188 - val_loss: 2.7824 - val_accuracy: 0.0762\n",
      "Epoch 30/30\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 2.4045 - accuracy: 0.1184 - val_loss: 2.7533 - val_accuracy: 0.0803\n",
      "Epoch 1/30\n",
      "313/313 [==============================] - 29s 92ms/step - loss: 2.7453 - accuracy: 0.1001 - val_loss: 2.7091 - val_accuracy: 0.1340\n",
      "Epoch 2/30\n",
      "313/313 [==============================] - 29s 92ms/step - loss: 2.6898 - accuracy: 0.1335 - val_loss: 2.7175 - val_accuracy: 0.1214\n",
      "Epoch 3/30\n",
      "313/313 [==============================] - 29s 92ms/step - loss: 2.6675 - accuracy: 0.1339 - val_loss: 2.6577 - val_accuracy: 0.1389\n",
      "Epoch 4/30\n",
      "313/313 [==============================] - 29s 92ms/step - loss: 2.6482 - accuracy: 0.1310 - val_loss: 2.6865 - val_accuracy: 0.1261\n",
      "Epoch 5/30\n",
      "313/313 [==============================] - 29s 92ms/step - loss: 2.6295 - accuracy: 0.1300 - val_loss: 2.6511 - val_accuracy: 0.1288\n",
      "Epoch 6/30\n",
      "313/313 [==============================] - 29s 92ms/step - loss: 2.6111 - accuracy: 0.1277 - val_loss: 2.6875 - val_accuracy: 0.1129\n",
      "Epoch 7/30\n",
      "313/313 [==============================] - 29s 91ms/step - loss: 2.5941 - accuracy: 0.1225 - val_loss: 2.6836 - val_accuracy: 0.1159\n",
      "Epoch 8/30\n",
      "313/313 [==============================] - 29s 92ms/step - loss: 2.5762 - accuracy: 0.1227 - val_loss: 2.7332 - val_accuracy: 0.0815\n",
      "Epoch 9/30\n",
      "313/313 [==============================] - 29s 92ms/step - loss: 2.5620 - accuracy: 0.1180 - val_loss: 2.6763 - val_accuracy: 0.1053\n",
      "Epoch 10/30\n",
      "313/313 [==============================] - 29s 92ms/step - loss: 2.5473 - accuracy: 0.1169 - val_loss: 2.7321 - val_accuracy: 0.0815\n",
      "Epoch 11/30\n",
      "313/313 [==============================] - 29s 92ms/step - loss: 2.5347 - accuracy: 0.1174 - val_loss: 2.6786 - val_accuracy: 0.0919\n",
      "Epoch 12/30\n",
      "313/313 [==============================] - 29s 92ms/step - loss: 2.5247 - accuracy: 0.1146 - val_loss: 2.7053 - val_accuracy: 0.0875\n",
      "Epoch 13/30\n",
      "313/313 [==============================] - 29s 92ms/step - loss: 2.5127 - accuracy: 0.1152 - val_loss: 2.6951 - val_accuracy: 0.0896\n",
      "Epoch 14/30\n",
      "313/313 [==============================] - 29s 92ms/step - loss: 2.5049 - accuracy: 0.1161 - val_loss: 2.7120 - val_accuracy: 0.0841\n",
      "Epoch 15/30\n",
      "313/313 [==============================] - 29s 92ms/step - loss: 2.4930 - accuracy: 0.1185 - val_loss: 2.7011 - val_accuracy: 0.0882\n",
      "Epoch 16/30\n",
      "313/313 [==============================] - 29s 92ms/step - loss: 2.4859 - accuracy: 0.1162 - val_loss: 2.7402 - val_accuracy: 0.0932\n",
      "Epoch 17/30\n",
      "313/313 [==============================] - 29s 92ms/step - loss: 2.4794 - accuracy: 0.1166 - val_loss: 2.7231 - val_accuracy: 0.0825\n",
      "Epoch 18/30\n",
      "313/313 [==============================] - 29s 92ms/step - loss: 2.4721 - accuracy: 0.1165 - val_loss: 2.7863 - val_accuracy: 0.0763\n",
      "Epoch 19/30\n",
      "313/313 [==============================] - 29s 92ms/step - loss: 2.4636 - accuracy: 0.1182 - val_loss: 2.7468 - val_accuracy: 0.0826\n",
      "Epoch 20/30\n",
      "313/313 [==============================] - 29s 92ms/step - loss: 2.4578 - accuracy: 0.1183 - val_loss: 2.7487 - val_accuracy: 0.0786\n",
      "Epoch 21/30\n",
      "313/313 [==============================] - 29s 92ms/step - loss: 2.4535 - accuracy: 0.1188 - val_loss: 2.7173 - val_accuracy: 0.0906\n",
      "Epoch 22/30\n",
      "313/313 [==============================] - 29s 92ms/step - loss: 2.4441 - accuracy: 0.1183 - val_loss: 2.7801 - val_accuracy: 0.0785\n",
      "Epoch 23/30\n",
      "313/313 [==============================] - 29s 93ms/step - loss: 2.4384 - accuracy: 0.1194 - val_loss: 2.7659 - val_accuracy: 0.0748\n",
      "Epoch 24/30\n",
      "313/313 [==============================] - 29s 93ms/step - loss: 2.4327 - accuracy: 0.1202 - val_loss: 2.7468 - val_accuracy: 0.0807\n",
      "Epoch 25/30\n",
      "313/313 [==============================] - 29s 93ms/step - loss: 2.4295 - accuracy: 0.1180 - val_loss: 2.8016 - val_accuracy: 0.0729\n",
      "Epoch 26/30\n",
      "313/313 [==============================] - 29s 93ms/step - loss: 2.4222 - accuracy: 0.1206 - val_loss: 2.7653 - val_accuracy: 0.0817\n",
      "Epoch 27/30\n",
      "313/313 [==============================] - 29s 94ms/step - loss: 2.4208 - accuracy: 0.1213 - val_loss: 2.7820 - val_accuracy: 0.0691\n",
      "Epoch 28/30\n",
      "313/313 [==============================] - 29s 93ms/step - loss: 2.4149 - accuracy: 0.1209 - val_loss: 2.7420 - val_accuracy: 0.0821\n",
      "Epoch 29/30\n",
      "313/313 [==============================] - 29s 92ms/step - loss: 2.4094 - accuracy: 0.1215 - val_loss: 2.7683 - val_accuracy: 0.0755\n",
      "Epoch 30/30\n",
      "313/313 [==============================] - 29s 92ms/step - loss: 2.4058 - accuracy: 0.1206 - val_loss: 2.8080 - val_accuracy: 0.0794\n"
     ]
    }
   ],
   "source": [
    "results_table_train = pd.DataFrame()\n",
    "results_table_test = pd.DataFrame()\n",
    "\n",
    "# Shuffle the training set so its not pre-sorted by language\n",
    "# from sklearn.utils import shuffle\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=0)\n",
    "\n",
    "for train_size in np.arange(0, 80001, 16000)[1:]:\n",
    "    train_X_fraction = X_train[:train_size]\n",
    "    train_y_fraction = y_train[:train_size]\n",
    "    \n",
    "    model = cnn_model(train_X_fraction, \n",
    "                      train_y_fraction, \n",
    "                      under_represented_weighting=weights,\n",
    "                      epochs_input=30,\n",
    "                      batch_size_input=256)\n",
    "    \n",
    "    train_acc = mbti_accuracy(train_y_fraction, model.predict(train_X_fraction))\n",
    "    test_acc = mbti_accuracy(y_test, model.predict(X_test))\n",
    "    results_table_train = results_table_train.append({'Number of Training Samples': train_size, \n",
    "                                                      'At Least 1 Match': train_acc[0],\n",
    "                                                      'At Least 2 Matches': train_acc[1],\n",
    "                                                      'At Least 3 Matches': train_acc[2],\n",
    "                                                      'Perfect Match': train_acc[3],\n",
    "                                                      'Average Match': train_acc[4]}, \n",
    "                                                     ignore_index=True)\n",
    "    \n",
    "    results_table_test = results_table_test.append({'Number of Training Samples': train_size, \n",
    "                                                      'At Least 1 Match': test_acc[0],\n",
    "                                                      'At Least 2 Matches': test_acc[1],\n",
    "                                                      'At Least 3 Matches': test_acc[2],\n",
    "                                                      'Perfect Match': test_acc[3],\n",
    "                                                      'Average Match': test_acc[4]}, \n",
    "                                                     ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>At Least 1 Match</th>\n",
       "      <th>At Least 2 Matches</th>\n",
       "      <th>At Least 3 Matches</th>\n",
       "      <th>Average Match</th>\n",
       "      <th>Number of Training Samples</th>\n",
       "      <th>Perfect Match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>77</td>\n",
       "      <td>44</td>\n",
       "      <td>55</td>\n",
       "      <td>16000</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>77</td>\n",
       "      <td>41</td>\n",
       "      <td>53</td>\n",
       "      <td>32000</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>77</td>\n",
       "      <td>41</td>\n",
       "      <td>54</td>\n",
       "      <td>48000</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>78</td>\n",
       "      <td>42</td>\n",
       "      <td>54</td>\n",
       "      <td>64000</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>77</td>\n",
       "      <td>42</td>\n",
       "      <td>53</td>\n",
       "      <td>80000</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   At Least 1 Match  At Least 2 Matches  At Least 3 Matches  Average Match  \\\n",
       "0               100                  77                  44             55   \n",
       "1               100                  77                  41             53   \n",
       "2               100                  77                  41             54   \n",
       "3               100                  78                  42             54   \n",
       "4               100                  77                  42             53   \n",
       "\n",
       "   Number of Training Samples  Perfect Match  \n",
       "0                       16000             23  \n",
       "1                       32000             18  \n",
       "2                       48000             18  \n",
       "3                       64000             18  \n",
       "4                       80000             18  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_table_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>At Least 1 Match</th>\n",
       "      <th>At Least 2 Matches</th>\n",
       "      <th>At Least 3 Matches</th>\n",
       "      <th>Average Match</th>\n",
       "      <th>Number of Training Samples</th>\n",
       "      <th>Perfect Match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>74</td>\n",
       "      <td>37</td>\n",
       "      <td>49</td>\n",
       "      <td>16000</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>74</td>\n",
       "      <td>36</td>\n",
       "      <td>50</td>\n",
       "      <td>32000</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>75</td>\n",
       "      <td>36</td>\n",
       "      <td>50</td>\n",
       "      <td>48000</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>77</td>\n",
       "      <td>38</td>\n",
       "      <td>52</td>\n",
       "      <td>64000</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>74</td>\n",
       "      <td>37</td>\n",
       "      <td>50</td>\n",
       "      <td>80000</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   At Least 1 Match  At Least 2 Matches  At Least 3 Matches  Average Match  \\\n",
       "0               100                  74                  37             49   \n",
       "1               100                  74                  36             50   \n",
       "2               100                  75                  36             50   \n",
       "3               100                  77                  38             52   \n",
       "4               100                  74                  37             50   \n",
       "\n",
       "   Number of Training Samples  Perfect Match  \n",
       "0                       16000             14  \n",
       "1                       32000             13  \n",
       "2                       48000             12  \n",
       "3                       64000             13  \n",
       "4                       80000             14  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_table_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table_train.to_csv('../NLP_MBTI_Classification/results_summary/cnn_sample_size_var_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table_test.to_csv('../NLP_MBTI_Classification/results_summary/cnn_sample_size_var_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
