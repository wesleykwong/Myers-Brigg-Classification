{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install simpletransformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom matplotlib import pyplot as plt\n\nfrom simpletransformers.classification import ClassificationModel, ClassificationArgs\n\nfrom sklearn.metrics import f1_score, accuracy_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Language: 'German', 'Italian', 'All', etc.\nlanguage = 'German'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Are you using Kaggle or another GPU?\ngpu_available = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ensures that tweet and user ids do not appear in scientific notation\npd.options.display.float_format = '{:.0f}'.format","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/twisty-samples/twisty_train.csv', index_col=0)\nif language !='All':\n    train = train[train['language'] == language]\ntrain","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/twisty-samples/twisty_test.csv', index_col=0)\nif language !='All':\n    test = test[test['language'] == language]\ntest","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Format Dataframe for Simple Transformers","metadata":{}},{"cell_type":"markdown","source":"Simple transformers wants the training and testing data to have a dataframe with two columns: 'text' and 'labels'. Text is the NLP twitter data and labels is the number encoding corresponding to the MBTI class.","metadata":{}},{"cell_type":"code","source":"mbti_num_encoding = {\n    'ISTJ':0, 'ISFJ':1, 'INFJ':2, 'INTJ':3,\n    'ISTP':4, 'ISFP':5, 'INFP':6, 'INTP':7,\n    'ESTP':8, 'ESFP':9, 'ENFP':10, 'ENTP':11,\n    'ESTJ':12, 'ESFJ':13, 'ENFJ':14, 'ENTJ':15}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['labels'] = train['mbti'].apply(lambda x: mbti_num_encoding[x])\ntest['labels'] = test['mbti'].apply(lambda x: mbti_num_encoding[x])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train[['twitter_text', 'labels']].rename(columns={'twitter_text':'text'})\ntest = test[['twitter_text', 'labels']].rename(columns={'twitter_text':'text'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['labels'].value_counts(normalize=True).plot.barh()\n\nplt.title('Distribution of MBTI Train')\nplt.xlabel('Percentage')\nplt.ylabel('MBTI');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['labels'].value_counts(normalize=True).plot.barh()\n\nplt.title('Distribution of MBTI Test')\nplt.xlabel('Percentage')\nplt.ylabel('MBTI');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **BERT Model**","metadata":{}},{"cell_type":"code","source":"weights_array = compute_class_weight('balanced', classes=np.arange(16), y=train['labels'])\nweights_array","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef BERT_model(train_df, epochs, bert_model=\"bert-base-german-cased\"):\n    \n    model_args = ClassificationArgs(\n        num_train_epochs=epochs,\n        learning_rate = 1e-4,\n        \n        # Twitter sequence length is less than 64 tokens\n        # https://peltarion.com/knowledge-center/documentation/cheat-sheets/bert---text-classification-/-cheat-sheet\n        train_batch_size = 64,\n        overwrite_output_dir=True\n    )\n    \n    model = ClassificationModel(\n        'bert', \n        bert_model, \n        use_cuda=gpu_available, #Set to true if using kaggle GPU\n        num_labels=16, \n        weight=list(weights_array), \n        args=model_args\n    )\n    \n    model.train_model(train_df)\n    #result, model_outputs, wrong_predictions = model.eval_model(test_df, acc=accuracy_score)\n    #print(result['acc'])\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Multilingual: \"bert-base-multilingual-cased\"**\n\n**German (DE): \"bert-base-german-cased\" https://huggingface.co/bert-base-german-cased**\n\n**Spanish (ES): \"dccuchile/bert-base-spanish-wwm-cased\" https://huggingface.co/dccuchile/bert-base-spanish-wwm-cased**\n\n**Italian (IT): \"dbmdz/bert-base-italian-cased\" https://huggingface.co/dbmdz/bert-base-italian-cased**\n\n**Dutch (NL): \"GroNLP/bert-base-dutch-cased\" https://huggingface.co/GroNLP/bert-base-dutch-cased**\n","metadata":{}},{"cell_type":"code","source":"all_bert_models = {'All': \"bert-base-multilingual-cased\",\n                  'German': \"bert-base-german-cased\",\n                  'Spanish': \"dccuchile/bert-base-spanish-wwm-cased\",\n                  'Italian': \"dbmdz/bert-base-italian-cased\",\n                  'Dutch': \"GroNLP/bert-base-dutch-cased\"}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# all_bert_models[language]\nbert = BERT_model(train, epochs=5, bert_model=\"bert-base-multilingual-cased\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model and Error Analysis","metadata":{}},{"cell_type":"code","source":"def mbti_accuracy(y_true, y_pred):\n    # Measures accuracy for mbti classification across 5 accuracy metrics:\n    # one match, two matches, three matches, perfect match, average match\n    \n    # Average match is number of letters match / 4\n    \n    # Comparing 'ENFJ' as the true class and 'ENFP' as the predicted class,\n    # this function returns...\n    # [1, 1, 1, 0, 0.75]\n     \n    # Get index of highest softmax/probability output in y_pred\n    # y_pred_index = np.argmax(y_pred, axis=1)\n    \n    # Use the index to identify the corresponding mbti class\n    mbti_num_encoding_list = list(mbti_num_encoding)\n    y_true_mbti = [mbti_num_encoding_list[idx] for idx in y_true]\n    y_pred_mbti = [mbti_num_encoding_list[idx] for idx in y_pred]\n    \n    one_match = []\n    two_matches = []\n    three_matches = []\n    perfect_match = []\n    \n    # Perform mbti accuracy measurements\n    sum_num_matches = 0\n    for i in np.arange(len(y_true_mbti)):\n        num_letter_matches = len(set(y_true_mbti[i]) & set(y_pred_mbti[i]))\n        \n        # At least 1 letter match\n        if num_letter_matches == 1:\n            one_match += [True]\n            two_matches += [False]\n            three_matches += [False]\n            perfect_match += [False]\n            \n        # At least 2 letter matches\n        elif num_letter_matches == 2:\n            one_match += [True]\n            two_matches += [True]\n            three_matches += [False]\n            perfect_match += [False]\n            \n        # At least 3 letter matches\n        elif num_letter_matches == 3:\n            one_match += [True]\n            two_matches += [True]\n            three_matches += [True]\n            perfect_match += [False]\n           \n        # Perfect match\n        else:\n            one_match += [True]\n            two_matches += [True]\n            three_matches += [True]\n            perfect_match += [True]\n        \n    # Average/partial matches\n        sum_num_matches += num_letter_matches\n    avg_num_matches = sum_num_matches/(len(y_true_mbti)*4)*100\n    \n    return np.round([np.mean(one_match)*100, \n                     np.mean(two_matches)*100, \n                     np.mean(three_matches)*100, \n                     np.mean(perfect_match)*100, \n                     avg_num_matches], \n                    2)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Kaggle has been having issues running this\n#train_predictions, train_raw_outputs = bert_german.predict(train['text'])\n\n#test_predictions, test_raw_outputs = bert_german.predict(test['text'])\n\ntrain_result, train_model_outputs, train_wrong_predictions = bert.eval_model(train)\n\ntest_result, test_model_outputs, test_wrong_predictions = bert.eval_model(test)\n\n\nfrom scipy.special import softmax\n\ntrain_predictions = np.argmax(softmax(train_model_outputs, axis=1), axis=1)\ntest_predictions = np.argmax(softmax(test_model_outputs, axis=1), axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example title: 'BERT-Base-German-Cased'\nbert_model_type = 'BERT-Base-Multilingual-'+language+'-Cased'\n\ntraining_acc_metrics = np.append(np.array(['Train', language, bert_model_type, train.shape[0]]), \n                                 mbti_accuracy(train['labels'], train_predictions))\ntesting_acc_metrics = np.append(np.array(['Test', language, bert_model_type, test.shape[0]]),\n                                mbti_accuracy(test['labels'], test_predictions))\n\nacc_metrics = np.vstack((training_acc_metrics, testing_acc_metrics))\n\nacc_metrics_summary = pd.DataFrame(acc_metrics, columns=['Data',\n                                                         'Language',\n                                                         'Model',\n                                                         'Number of Samples',\n                                                         'At Least 1 Match', \n                                                         'At Least 2 Matches', \n                                                         'At Least 3 Matches', \n                                                         'Perfect Match', \n                                                         'Average Match'])\nacc_metrics_summary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc_metrics_summary.to_csv('bert_multilingual_'+language.lower()+'_summary.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cf_matrix = confusion_matrix(train['labels'], train_predictions, normalize='true')\n\n\nplt.figure(figsize=(12, 12))\nsns.heatmap(cf_matrix, annot=True, xticklabels=list(mbti_num_encoding), yticklabels=list(mbti_num_encoding), vmin=0, vmax=1)\nplt.title(\"MBTI Classifications of Multilingual \"+language+\" BERT Train\")\nplt.ylabel(\"Actual MBTI Classifications\")\nplt.xlabel(\"Predicted MBTI Classifications\");\n\n#plt.savefig('bert_german_train_confusion_matrix.png', transparent=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cf_matrix = confusion_matrix(test['labels'], test_predictions, normalize='true')\n\n\nplt.figure(figsize=(12, 12))\nsns.heatmap(cf_matrix, annot=True, xticklabels=list(mbti_num_encoding), yticklabels=list(mbti_num_encoding), vmin=0, vmax=1)\nplt.title(\"MBTI Classifications of Multilingual \"+language+\" BERT Test\")\nplt.ylabel(\"Actual MBTI Classifications\")\nplt.xlabel(\"Predicted MBTI Classifications\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}